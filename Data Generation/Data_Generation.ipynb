{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nf5bQw9lZTzN"
   },
   "source": [
    "# Bachelor's thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import roc_auc_score,roc_curve, brier_score_loss\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import mannwhitneyu\n",
    "from scipy.stats import ks_2samp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nf5bQw9lZTzN"
   },
   "source": [
    "### Loading the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "54Ud3XtqZTzN"
   },
   "outputs": [],
   "source": [
    "unbiased_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_data = pd.read_csv('../data/accepts.csv',encoding = \"ISO-8859-1\", low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rebalancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this step I rebalance my data so that it has a sensible amount of good and bad loan to represent an unbiased set. First I scale the numerical features and then use SMOTE to resample. You can try other resampling methods, as you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "scaler.fit(unbiased_data[['loan_amnt','dti', 'fico','int_rate',\"bc_open_to_buy\",\"acc_open_past_24mths\"]])\n",
    "unbiased_data[['loan_amnt', 'dti', 'fico','int_rate',\"bc_open_to_buy\",\"acc_open_past_24mths\"]] = scaler.transform(unbiased_data[['loan_amnt','dti', 'fico','int_rate',\"bc_open_to_buy\",\"acc_open_past_24mths\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = unbiased_data.drop(\"loan_status\", axis=1)\n",
    "y = unbiased_data[\"loan_status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42,sampling_strategy = 0.9)\n",
    "X_unbiased, y_unbiased = sm.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_data = pd.concat([X_unbiased, y_unbiased], axis = 1)\n",
    "unbiased_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now I inverse the scaling and change the values slightly. For example FICO Scores used in our data only end on 0 or 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_data[['loan_amnt', 'dti', 'fico','int_rate',\"bc_open_to_buy\",\"acc_open_past_24mths\"]] = scaler.inverse_transform(unbiased_data[['loan_amnt', 'dti', 'fico','int_rate',\"bc_open_to_buy\",\"acc_open_past_24mths\"]], copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_convert = ['loan_amnt', 'fico','bc_open_to_buy','acc_open_past_24mths']\n",
    "unbiased_data[columns_to_convert] = unbiased_data[columns_to_convert].round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to_5_or_10(x):\n",
    "    return 5 * round(x / 5)\n",
    "\n",
    "unbiased_data['fico'] = unbiased_data['fico'].apply(round_to_5_or_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to_25_intervals(x):\n",
    "    return 25 * round(x / 25)\n",
    "\n",
    "unbiased_data['loan_amnt'] = unbiased_data['loan_amnt'].apply(round_to_25_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scorecard to predict Accepts and Rejects "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I use a simple XGB-Classifier to predict the probabilities of default and then to assign them to the accepts or rejects. You can also drop the variables int_rate, bc_open_to_buy, acc_open_past_24mths, to see how well the model with only observed variables would do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = unbiased_data.drop(\"loan_status\", axis=1)\n",
    "y = unbiased_data[\"loan_status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = XGBClassifier(n_estimators=150, learning_rate=1, max_depth=3, random_state=42)\n",
    "xgb_clf.fit(X, y)\n",
    "yhat = xgb_clf.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are the results of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(y, yhat[:,1])\n",
    "print(\"AUC:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brier = brier_score_loss(y, yhat[:,1],pos_label=1)\n",
    "print(\"Brier score:\", brier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y, yhat[:,1],pos_label=1) \n",
    "ks_statistic = max(tpr - fpr)\n",
    "print(\"KS-Statistic:\",ks_statistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now I assign my observations to the accepts or rejects based on the percentiles of the predictions and thus have my new accepts and new rejects. I also drop the unobserved variables, so that the later compared reject inference models don't have access to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_data['predictions'] = yhat[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_data = unbiased_data.drop(\"int_rate\", axis=1)\n",
    "unbiased_data = unbiased_data.drop(\"bc_open_to_buy\", axis=1)\n",
    "unbiased_data = unbiased_data.drop(\"acc_open_past_24mths\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accept_percentile = np.percentile(unbiased_data['predictions'], 70)  # Adjust percentile as required\n",
    "reject_percentile = np.percentile(unbiased_data['predictions'], 70)\n",
    "New_accepts = unbiased_data[unbiased_data['predictions'] >= accept_percentile]\n",
    "New_rejects = unbiased_data[unbiased_data['predictions']< reject_percentile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_rejects = New_rejects.drop('predictions', axis=1)\n",
    "New_accepts = New_accepts.drop('predictions', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I also drop Duplicates which makes it easier to analyse the models later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_acc = New_accepts.copy()\n",
    "X_rej = New_rejects.copy()\n",
    "X_acc['Rejected']=0\n",
    "X_rej['Rejected']=1\n",
    "All_data = pd.concat([X_acc, X_rej])\n",
    "All_data = All_data.drop_duplicates(subset=['loan_amnt', 'emp_length','dti','fico','addr_state'], keep=False)\n",
    "New_accepts = All_data[All_data['Rejected'] == 0]\n",
    "New_rejects = All_data[All_data['Rejected'] == 1]\n",
    "New_accepts = New_accepts.drop('Rejected', axis=1)\n",
    "New_rejects = New_rejects.drop('Rejected', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are our final accepts and rejects and their values for loan_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_accepts.value_counts(['loan_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_rejects.value_counts(['loan_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data.value_counts(['loan_status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we save these newly created accepts and rejects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_rejects.to_csv('../data/New_rejects.csv', encoding='utf-8', index=False)\n",
    "New_accepts.to_csv('../data/New_accepts.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now I show some statistics about the created accepts and rejects, as well as performing some statistical tests. Here you can see the Mann-Whitney-U-Test for the variable loan amount as an example. I used the statistic to calculate my effect sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_accepts.describe().apply(lambda x: x.apply('{0:.1f}'.format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_rejects.describe().apply(lambda x: x.apply('{0:.1f}'.format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mannwhitneyu(New_accepts.iloc[:,0], New_rejects.iloc[:,0])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "4_ex_data_prep.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
