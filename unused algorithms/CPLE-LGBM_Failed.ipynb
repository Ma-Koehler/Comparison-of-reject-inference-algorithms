{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nf5bQw9lZTzN"
   },
   "source": [
    "# Bachelor's thesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The corresponding paper is: Xia, Yufei, Xiaoli Yang, and Yeying Zhang. \"A rejection inference technique based on contrastive pessimistic likelihood estimation for P2P lending.\" Electronic Commerce Research and Applications 30 (2018): 111-124."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Implementation is based on the Github implementation \"semisup-learn\" from tmadl. This is the exact implementation: semisup-learn/frameworks/CPLELearning.py \n",
    "(Reference: The MIT License (MIT) Copyright (c) 2015 Tamas Madl\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden von Standard-Bibliotheken\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unbuffered(object):\n",
    "    def __init__(self, stream):\n",
    "        self.stream = stream\n",
    "    def write(self, data):\n",
    "        self.stream.write(data)\n",
    "        self.stream.flush()\n",
    "    def __getattr__(self, attr):\n",
    "        return getattr(self.stream, attr)\n",
    "\n",
    "import sys\n",
    "sys.stdout = Unbuffered(sys.stdout)\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "import numpy\n",
    "import sklearn.metrics\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "import nlopt\n",
    "import scipy.stats\n",
    "\n",
    "class CPLELearningModel(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Contrastive Pessimistic Likelihood Estimation framework for semi-supervised\n",
    "    learning, based on (Loog, 2015). This implementation contains two\n",
    "    significant differences to (Loog, 2015):\n",
    "    - the discriminative likelihood p(y|X), instead of the generative\n",
    "    likelihood p(X), is used for optimization\n",
    "    - apart from `pessimism' (the assumption that the true labels of the\n",
    "    unlabeled instances are as adversarial to the likelihood as possible), the\n",
    "    optimization objective also tries to increase the likelihood on the labeled\n",
    "    examples\n",
    "\n",
    "    This class takes a base model (any scikit learn estimator),\n",
    "    trains it on the labeled examples, and then uses global optimization to\n",
    "    find (soft) label hypotheses for the unlabeled examples in a pessimistic\n",
    "    fashion (such that the model log likelihood on the unlabeled data is as\n",
    "    small as possible, but the log likelihood on the labeled data is as high\n",
    "    as possible)\n",
    "\n",
    "    See Loog, Marco. \"Contrastive Pessimistic Likelihood Estimation for\n",
    "    Semi-Supervised Classification.\" arXiv preprint arXiv:1503.00269 (2015).\n",
    "    http://arxiv.org/pdf/1503.00269\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    basemodel : BaseEstimator instance\n",
    "        Base classifier to be trained on the partially supervised data\n",
    "\n",
    "    pessimistic : boolean, optional (default=True)\n",
    "        Whether the label hypotheses for the unlabeled instances should be\n",
    "        pessimistic (i.e. minimize log likelihood) or optimistic (i.e.\n",
    "        maximize log likelihood).\n",
    "        Pessimistic label hypotheses ensure safety (i.e. the semi-supervised\n",
    "        solution will not be worse than a model trained on the purely\n",
    "        supervised instances)\n",
    "\n",
    "    predict_from_probabilities : boolean, optional (default=False)\n",
    "        The prediction is calculated from the probabilities if this is True\n",
    "        (1 if more likely than the mean predicted probability or 0 otherwise).\n",
    "        If it is false, the normal base model predictions are used.\n",
    "        This only affects the predict function. Warning: only set to true if\n",
    "        predict will be called with a substantial number of data points\n",
    "\n",
    "    use_sample_weighting : boolean, optional (default=True)\n",
    "        Whether to use sample weights (soft labels) for the unlabeled instances.\n",
    "        Setting this to False allows the use of base classifiers which do not\n",
    "        support sample weights (but might slow down the optimization)\n",
    "\n",
    "    max_iter : int, optional (default=3000)\n",
    "        Maximum number of iterations\n",
    "\n",
    "    verbose : int, optional (default=1)\n",
    "        Enable verbose output (1 shows progress, 2 shows the detailed log\n",
    "        likelihood at every iteration).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, basemodel, pessimistic=True, predict_from_probabilities = False, use_sample_weighting = True, max_iter=3000, verbose = 1):\n",
    "        self.model = basemodel\n",
    "        self.pessimistic = pessimistic\n",
    "        self.predict_from_probabilities = predict_from_probabilities\n",
    "        self.use_sample_weighting = use_sample_weighting\n",
    "        self.max_iter = max_iter\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.it = 0 # iteration counter\n",
    "        self.noimprovementsince = 0 # log likelihood hasn't improved since this number of iterations\n",
    "        self.maxnoimprovementsince = 3 # threshold for iterations without improvements (convergence is assumed when this is reached)\n",
    "\n",
    "        self.buffersize = 200\n",
    "        # buffer for the last few discriminative likelihoods (used to check for convergence)\n",
    "        self.lastdls = [0]*self.buffersize\n",
    "\n",
    "        # best discriminative likelihood and corresponding soft labels; updated during training\n",
    "        self.bestdl = numpy.infty\n",
    "        self.bestlbls = []\n",
    "\n",
    "        # unique id\n",
    "        self.id = str(chr(numpy.random.randint(26)+97))+str(chr(numpy.random.randint(26)+97))\n",
    "\n",
    "    def discriminative_likelihood(self, model, labeledData, labeledy = None, unlabeledData = None, unlabeledWeights = None, unlabeledlambda = 1, gradient=[], alpha = 0.01):\n",
    "        unlabeledy = (unlabeledWeights[:, 0]<0.5)*1\n",
    "        uweights = numpy.copy(unlabeledWeights[:, 0]) # large prob. for k=0 instances, small prob. for k=1 instances\n",
    "        uweights[unlabeledy==1] = 1-uweights[unlabeledy==1] # subtract from 1 for k=1 instances to reflect confidence\n",
    "        weights = numpy.hstack((numpy.ones(len(labeledy)), uweights))\n",
    "        labels = numpy.hstack((labeledy, unlabeledy))\n",
    "\n",
    "        # fit model on supervised data\n",
    "        if self.use_sample_weighting:\n",
    "            model.fit(numpy.vstack((labeledData, unlabeledData)), labels, sample_weight=weights)\n",
    "        else:\n",
    "            model.fit(numpy.vstack((labeledData, unlabeledData)), labels)\n",
    "\n",
    "        # probability of labeled data\n",
    "        P = model.predict_proba(labeledData)\n",
    "\n",
    "        try:\n",
    "            # labeled discriminative log likelihood\n",
    "            labeledDL = -sklearn.metrics.log_loss(labeledy, P)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            P = model.predict_proba(labeledData)\n",
    "\n",
    "        # probability of unlabeled data\n",
    "        unlabeledP = model.predict_proba(unlabeledData)\n",
    "\n",
    "        try:\n",
    "            # unlabeled discriminative log likelihood\n",
    "            eps = 1e-15\n",
    "            unlabeledP = numpy.clip(unlabeledP, eps, 1 - eps)\n",
    "            unlabeledDL = numpy.average((unlabeledWeights*numpy.vstack((1-unlabeledy, unlabeledy)).T*numpy.log(unlabeledP)).sum(axis=1))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            unlabeledP = model.predict_proba(unlabeledData)\n",
    "\n",
    "        if self.pessimistic:\n",
    "            # pessimistic: minimize the difference between unlabeled and labeled discriminative likelihood (assume worst case for unknown true labels)\n",
    "            dl = unlabeledlambda * unlabeledDL - labeledDL\n",
    "        else:\n",
    "            # optimistic: minimize negative total discriminative likelihood (i.e. maximize likelihood)\n",
    "            dl = - unlabeledlambda * unlabeledDL - labeledDL\n",
    "\n",
    "        return dl\n",
    "\n",
    "    def discriminative_likelihood_objective(self, model, labeledData, labeledy = None, unlabeledData = None, unlabeledWeights = None, unlabeledlambda = 1, gradient=[], alpha = 0.01):\n",
    "        if self.it == 0:\n",
    "            self.lastdls = [0]*self.buffersize\n",
    "\n",
    "        dl = self.discriminative_likelihood(model, labeledData, labeledy, unlabeledData, unlabeledWeights, unlabeledlambda, gradient, alpha)\n",
    "\n",
    "        self.it += 1\n",
    "        self.lastdls[numpy.mod(self.it, len(self.lastdls))] = dl\n",
    "\n",
    "        if numpy.mod(self.it, self.buffersize) == 0: # or True:\n",
    "            improvement = numpy.mean((self.lastdls[(len(self.lastdls)/2):])) - numpy.mean((self.lastdls[:(len(self.lastdls)/2)]))\n",
    "            # ttest - test for hypothesis that the likelihoods have not changed (i.e. there has been no improvement, and we are close to convergence)\n",
    "            _, prob = scipy.stats.ttest_ind(self.lastdls[(len(self.lastdls)/2):], self.lastdls[:(len(self.lastdls)/2)])\n",
    "\n",
    "            # if improvement is not certain accoring to t-test...\n",
    "            noimprovement = prob > 0.1 and numpy.mean(self.lastdls[(len(self.lastdls)/2):]) < numpy.mean(self.lastdls[:(len(self.lastdls)/2)])\n",
    "            if noimprovement:\n",
    "                self.noimprovementsince += 1\n",
    "                if self.noimprovementsince >= self.maxnoimprovementsince:\n",
    "                    # no improvement since a while - converged; exit\n",
    "                    self.noimprovementsince = 0\n",
    "                    raise Exception(\" converged.\") # we need to raise an exception to get NLopt to stop before exceeding the iteration budget\n",
    "            else:\n",
    "                self.noimprovementsince = 0\n",
    "\n",
    "            if self.verbose == 2:\n",
    "                print(self.id,self.it, dl, numpy.mean(self.lastdls), improvement, round(prob, 3), (prob < 0.1))\n",
    "            elif self.verbose:\n",
    "                sys.stdout.write(('.' if self.pessimistic else '.') if not noimprovement else 'n')\n",
    "\n",
    "        if dl < self.bestdl:\n",
    "            self.bestdl = dl\n",
    "            self.bestlbls = numpy.copy(unlabeledWeights[:, 0])\n",
    "\n",
    "        return dl\n",
    "\n",
    "    def fit(self, X, y): # -1 for unlabeled\n",
    "        unlabeledX = X[y==-1, :]\n",
    "        labeledX = X[y!=-1, :]\n",
    "        labeledy = y[y!=-1]\n",
    "\n",
    "        M = unlabeledX.shape[0]\n",
    "\n",
    "        # train on labeled data\n",
    "        self.model.fit(labeledX, labeledy)\n",
    "\n",
    "        unlabeledy = self.predict(unlabeledX)\n",
    "\n",
    "        #re-train, labeling unlabeled instances pessimistically\n",
    "\n",
    "        # pessimistic soft labels ('weights') q for unlabelled points, q=P(k=0|Xu)\n",
    "        f = lambda softlabels, grad=[]: self.discriminative_likelihood_objective(self.model, labeledX, labeledy=labeledy, unlabeledData=unlabeledX, unlabeledWeights=numpy.vstack((softlabels, 1-softlabels)).T, gradient=grad) #- supLL\n",
    "        lblinit = numpy.random.random(len(unlabeledy))\n",
    "\n",
    "        try:\n",
    "            self.it = 0\n",
    "            opt = nlopt.opt(nlopt.GN_DIRECT_L_RAND, M)\n",
    "            opt.set_lower_bounds(numpy.zeros(M))\n",
    "            opt.set_upper_bounds(numpy.ones(M))\n",
    "            opt.set_min_objective(f)\n",
    "            opt.set_maxeval(self.max_iter)\n",
    "            self.bestsoftlbl = opt.optimize(lblinit)\n",
    "            print(\" max_iter exceeded.\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            self.bestsoftlbl = self.bestlbls\n",
    "\n",
    "        if numpy.any(self.bestsoftlbl != self.bestlbls):\n",
    "            self.bestsoftlbl = self.bestlbls\n",
    "        ll = f(self.bestsoftlbl)\n",
    "\n",
    "        unlabeledy = (self.bestsoftlbl<0.5)*1\n",
    "        uweights = numpy.copy(self.bestsoftlbl) # large prob. for k=0 instances, small prob. for k=1 instances\n",
    "        uweights[unlabeledy==1] = 1-uweights[unlabeledy==1] # subtract from 1 for k=1 instances to reflect confidence\n",
    "        weights = numpy.hstack((numpy.ones(len(labeledy)), uweights))\n",
    "        labels = numpy.hstack((labeledy, unlabeledy))\n",
    "        if self.use_sample_weighting:\n",
    "            self.model.fit(numpy.vstack((labeledX, unlabeledX)), labels, sample_weight=weights)\n",
    "        else:\n",
    "            self.model.fit(numpy.vstack((labeledX, unlabeledX)), labels)\n",
    "\n",
    "        if self.verbose > 1:\n",
    "            print(\"number of non-one soft labels: \", numpy.sum(self.bestsoftlbl != 1), \", balance:\", numpy.sum(self.bestsoftlbl<0.5), \" / \", len(self.bestsoftlbl))\n",
    "            print(\"current likelihood: \", ll)\n",
    "\n",
    "        if not getattr(self.model, \"predict_proba\", None):\n",
    "            # Platt scaling\n",
    "            self.plattlr = LR()\n",
    "            preds = self.model.predict(labeledX)\n",
    "            self.plattlr.fit( preds.reshape( -1, 1 ), labeledy )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Compute probabilities of possible outcomes for samples in X.\n",
    "\n",
    "        The model need to have probability information computed at training\n",
    "        time: fit with attribute `probability` set to True.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        T : array-like, shape = [n_samples, n_classes]\n",
    "            Returns the probability of the sample for each class in\n",
    "            the model. The columns correspond to the classes in sorted\n",
    "            order, as they appear in the attribute `classes_`.\n",
    "        \"\"\"\n",
    "\n",
    "        if getattr(self.model, \"predict_proba\", None):\n",
    "            return self.model.predict_proba(X)\n",
    "        else:\n",
    "            preds = self.model.predict(X)\n",
    "            return self.plattlr.predict_proba(preds.reshape( -1, 1 ))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Perform classification on samples in X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : array, shape = [n_samples]\n",
    "            Class labels for samples in X.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.predict_from_probabilities:\n",
    "            P = self.predict_proba(X)\n",
    "            return (P[:, 0]<numpy.average(P[:, 0]))\n",
    "        else:\n",
    "            return self.model.predict(X)\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        return sklearn.metrics.accuracy_score(y, self.predict(X), sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nf5bQw9lZTzN"
   },
   "source": [
    "## Loading the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepts = pd.DataFrame()\n",
    "accepts = pd.read_csv('../data/New_accepts.csv',encoding = \"ISO-8859-1\", low_memory=False)\n",
    "rejects = pd.DataFrame()\n",
    "rejects = pd.read_csv('../data/New_rejects.csv',encoding = \"ISO-8859-1\", low_memory=False)\n",
    "X_acc = accepts.copy()\n",
    "X_rej = rejects.copy()\n",
    "y_rej = X_rej.pop(\"loan_status\")\n",
    "y_acc = X_acc.pop(\"loan_status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLPE-LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sadly the implementation always throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_acc.to_numpy()\n",
    "y_test = y_acc.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssmodel = CPLELearningModel(lgb.LGBMClassifier(), predict_from_probabilities=True) # RBF SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 93530, number of negative: 459005\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003917 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 564\n",
      "[LightGBM] [Info] Number of data points in the train set: 552535, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.169274 -> initscore=-1.590779\n",
      "[LightGBM] [Info] Start training from score -1.590779\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 5)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mssmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 190\u001b[0m, in \u001b[0;36mCPLELearningModel.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# train on labeled data\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit(labeledX, labeledy)\n\u001b[0;32m--> 190\u001b[0m unlabeledy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43munlabeledX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m#re-train, labeling unlabeled instances pessimistically\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# pessimistic soft labels ('weights') q for unlabelled points, q=P(k=0|Xu)\u001b[39;00m\n\u001b[1;32m    195\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m softlabels, grad\u001b[38;5;241m=\u001b[39m[]: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminative_likelihood_objective(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, labeledX, labeledy\u001b[38;5;241m=\u001b[39mlabeledy, unlabeledData\u001b[38;5;241m=\u001b[39munlabeledX, unlabeledWeights\u001b[38;5;241m=\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mvstack((softlabels, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39msoftlabels))\u001b[38;5;241m.\u001b[39mT, gradient\u001b[38;5;241m=\u001b[39mgrad) \u001b[38;5;66;03m#- supLL\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 275\u001b[0m, in \u001b[0;36mCPLELearningModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Perform classification on samples in X.\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m    Class labels for samples in X.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_from_probabilities:\n\u001b[0;32m--> 275\u001b[0m     P \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (P[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m<\u001b[39mnumpy\u001b[38;5;241m.\u001b[39maverage(P[:, \u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[2], line 256\u001b[0m, in \u001b[0;36mCPLELearningModel.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute probabilities of possible outcomes for samples in X.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03mThe model need to have probability information computed at training\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03m    order, as they appear in the attribute `classes_`.\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict_proba\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(X)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightgbm/sklearn.py:1351\u001b[0m, in \u001b[0;36mLGBMClassifier.predict_proba\u001b[0;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, validate_features, **kwargs)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_proba\u001b[39m(\n\u001b[1;32m   1340\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1341\u001b[0m     X: _LGBM_ScikitMatrixLike,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1349\u001b[0m ):\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Docstring is set after definition, using a template.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1351\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraw_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpred_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_leaf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpred_contrib\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_contrib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1361\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_objective) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (raw_score \u001b[38;5;129;01mor\u001b[39;00m pred_leaf \u001b[38;5;129;01mor\u001b[39;00m pred_contrib):\n\u001b[1;32m   1362\u001b[0m         _log_warning(\n\u001b[1;32m   1363\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot compute class probabilities or labels \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1364\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdue to the usage of customized objective function.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1365\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning raw scores instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1366\u001b[0m         )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightgbm/sklearn.py:1007\u001b[0m, in \u001b[0;36mLGBMModel.predict\u001b[0;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, validate_features, **kwargs)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LGBMNotFittedError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEstimator not fitted, call fit before exploiting the model.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, (pd_DataFrame, dt_DataTable)):\n\u001b[0;32m-> 1007\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43m_LGBMCheckArray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1008\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_features \u001b[38;5;241m!=\u001b[39m n_features:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/validation.py:967\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    965\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[0;32m--> 967\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    968\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    969\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    970\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m    971\u001b[0m         )\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    974\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 5)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "ssmodel.fit(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "4_ex_data_prep.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
